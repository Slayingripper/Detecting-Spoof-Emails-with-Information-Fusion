[34m[1mwandb[0m: [33mWARNING[0m wandb.init() arguments ignored because wandb magic has already been initialized
Reading GloVe: 0it [00:00, ?it/s]Reading GloVe: 4736it [00:00, 47357.11it/s]Reading GloVe: 10532it [00:00, 50100.54it/s]Reading GloVe: 16002it [00:00, 51395.62it/s]Reading GloVe: 21691it [00:00, 52928.98it/s]Reading GloVe: 27590it [00:00, 54612.04it/s]Reading GloVe: 33496it [00:00, 55874.01it/s]Reading GloVe: 39283it [00:00, 56457.98it/s]Reading GloVe: 44566it [00:00, 54886.71it/s]Reading GloVe: 50455it [00:00, 56028.27it/s]Reading GloVe: 56232it [00:01, 56539.36it/s]Reading GloVe: 62074it [00:01, 57087.98it/s]Reading GloVe: 68118it [00:01, 58051.99it/s]Reading GloVe: 73872it [00:01, 57355.87it/s]Reading GloVe: 79677it [00:01, 57560.99it/s]Reading GloVe: 85428it [00:01, 57541.38it/s]Reading GloVe: 91166it [00:01, 57129.72it/s]Reading GloVe: 97086it [00:01, 57734.05it/s]Reading GloVe: 102880it [00:01, 57794.79it/s]Reading GloVe: 108656it [00:01, 54983.78it/s]Reading GloVe: 114181it [00:02, 53625.35it/s]Reading GloVe: 119569it [00:02, 53534.05it/s]Reading GloVe: 125306it [00:02, 54561.17it/s]Reading GloVe: 130913it [00:02, 55002.62it/s]Reading GloVe: 136427it [00:02, 53401.22it/s]Reading GloVe: 142438it [00:02, 55249.37it/s]Reading GloVe: 148293it [00:02, 56199.56it/s]Reading GloVe: 153938it [00:02, 56244.31it/s]Reading GloVe: 159884it [00:02, 57171.05it/s]Reading GloVe: 165824it [00:02, 57821.58it/s]Reading GloVe: 171825it [00:03, 58459.26it/s]Reading GloVe: 177682it [00:03, 53892.79it/s]Reading GloVe: 183611it [00:03, 55402.97it/s]Reading GloVe: 189386it [00:03, 56086.39it/s]Reading GloVe: 195242it [00:03, 56723.81it/s]Reading GloVe: 200950it [00:03, 55410.97it/s]Reading GloVe: 206523it [00:03, 54696.66it/s]Reading GloVe: 212387it [00:03, 55623.02it/s]Reading GloVe: 218019it [00:03, 55829.84it/s]Reading GloVe: 223780it [00:03, 56350.67it/s]Reading GloVe: 229542it [00:04, 56725.01it/s]Reading GloVe: 235234it [00:04, 56781.16it/s]Reading GloVe: 241141it [00:04, 57307.62it/s]Reading GloVe: 246878it [00:04, 57200.75it/s]Reading GloVe: 252836it [00:04, 57894.02it/s]Reading GloVe: 258668it [00:04, 58020.52it/s]Reading GloVe: 264474it [00:04, 57912.02it/s]Reading GloVe: 270375it [00:04, 58234.24it/s]Reading GloVe: 276312it [00:04, 58570.31it/s]Reading GloVe: 282225it [00:04, 58735.57it/s]Reading GloVe: 288101it [00:05, 58655.05it/s]Reading GloVe: 293968it [00:05, 58380.16it/s]Reading GloVe: 299808it [00:05, 56447.49it/s]Reading GloVe: 305775it [00:05, 57375.53it/s]Reading GloVe: 311566it [00:05, 57534.44it/s]Reading GloVe: 317441it [00:05, 57678.83it/s]Reading GloVe: 323216it [00:05, 57580.40it/s]Reading GloVe: 329054it [00:05, 57817.21it/s]Reading GloVe: 334840it [00:05, 57592.25it/s]Reading GloVe: 340602it [00:06, 56921.43it/s]Reading GloVe: 346521it [00:06, 57582.74it/s]Reading GloVe: 352284it [00:06, 53762.90it/s]Reading GloVe: 358049it [00:06, 54870.93it/s]Reading GloVe: 363835it [00:06, 55733.57it/s]Reading GloVe: 369804it [00:06, 56702.06it/s]Reading GloVe: 375812it [00:06, 57673.14it/s]Reading GloVe: 381603it [00:06, 57559.08it/s]Reading GloVe: 387376it [00:06, 57454.92it/s]Reading GloVe: 393134it [00:06, 57469.25it/s]Reading GloVe: 398890it [00:07, 53505.51it/s]Reading GloVe: 400000it [00:07, 56341.64it/s]
2020-02-18 12:05:21.566869: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-18 12:05:21.590757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494395000 Hz
2020-02-18 12:05:21.591168: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1536450 executing computations on platform Host. Devices:
2020-02-18 12:05:21.591213: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-02-18 12:05:21.596562: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2020-02-18 12:05:21.606339: W tensorflow/core/framework/allocator.cc:107] Allocation of 42534800 exceeds 10% of system memory.
2020-02-18 12:05:21.717423: W tensorflow/core/framework/allocator.cc:107] Allocation of 42534800 exceeds 10% of system memory.
2020-02-18 12:05:21.739357: W tensorflow/core/framework/allocator.cc:107] Allocation of 42534800 exceeds 10% of system memory.
tracking <tf.Variable 'Variable:0' shape=() dtype=int32> tp
tracking <tf.Variable 'Variable_1:0' shape=() dtype=int32> fp
tracking <tf.Variable 'Variable_2:0' shape=() dtype=int32> tp
tracking <tf.Variable 'Variable_3:0' shape=() dtype=int32> fn
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 50, 50)            10633700  
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               91648     
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 258       
=================================================================
Total params: 10,725,606
Trainable params: 91,906
Non-trainable params: 10,633,700
_________________________________________________________________
X_train.shape: (225000, 50)
X_test.shape: (75000, 50)
y_train.shape: (225000, 2)
y_test.shape: (75000, 2)
WARNING:tensorflow:From /home/blackfalcon/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/blackfalcon/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 225000 samples, validate on 75000 samples
WARNING:tensorflow:From /home/blackfalcon/.local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /home/blackfalcon/.local/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Epoch 1/20
2020-02-18 12:05:24.878745: W tensorflow/core/framework/allocator.cc:107] Allocation of 42534800 exceeds 10% of system memory.
   512/225000 [..............................] - ETA: 5:35 - loss: 0.6903 - accuracy: 0.5840 - precision: 0.2824 - recall: 0.1968  1024/225000 [..............................] - ETA: 4:02 - loss: 0.6812 - accuracy: 0.6514 - precision: 0.3474 - recall: 0.2674  1536/225000 [..............................] - ETA: 3:28 - loss: 0.6728 - accuracy: 0.6901 - precision: 0.3954 - recall: 0.3238  2048/225000 [..............................] - ETA: 3:10 - loss: 0.6637 - accuracy: 0.7114 - precision: 0.4328 - recall: 0.3724  2560/225000 [..............................] - ETA: 2:59 - loss: 0.6557 - accuracy: 0.7219 - precision: 0.4588 - recall: 0.4125  3072/225000 [..............................] - ETA: 2:52 - loss: 0.6464 - accuracy: 0.7432 - precision: 0.4825 - recall: 0.4453  3584/225000 [..............................] - ETA: 2:46 - loss: 0.6374 - accuracy: 0.7559 - precision: 0.5028 - recall: 0.4721  4096/225000 [..............................] - ETA: 2:42 - loss: 0.6292 - accuracy: 0.7654 - precision: 0.5207 - recall: 0.4940  4608/225000 [..............................] - ETA: 2:39 - loss: 0.6210 - accuracy: 0.7793 - precision: 0.5376 - recall: 0.5121  5120/225000 [..............................] - ETA: 2:36 - loss: 0.6111 - accuracy: 0.7937 - precision: 0.5534 - recall: 0.5267  5632/225000 [..............................] - ETA: 2:35 - loss: 0.6018 - accuracy: 0.8024 - precision: 0.5672 - recall: 0.5362  6144/225000 [..............................] - ETA: 2:33 - loss: 0.5918 - accuracy: 0.7996 - precision: 0.5790 - recall: 0.5401  6656/225000 [..............................] - ETA: 2:32 - loss: 0.5822 - accuracy: 0.7970 - precision: 0.5890 - recall: 0.5403  7168/225000 [..............................] - ETA: 2:30 - loss: 0.5729 - accuracy: 0.7976 - precision: 0.5977 - recall: 0.5385  7680/225000 [>.............................] - ETA: 2:29 - loss: 0.5634 - accuracy: 0.8040 - precision: 0.6058 - recall: 0.5358  8192/225000 [>.............................] - ETA: 2:28 - loss: 0.5557 - accuracy: 0.8081 - precision: 0.6135 - recall: 0.5332  8704/225000 [>.............................] - ETA: 2:27 - loss: 0.5474 - accuracy: 0.8138 - precision: 0.6208 - recall: 0.5312  9216/225000 [>.............................] - ETA: 2:26 - loss: 0.5383 - accuracy: 0.8190 - precision: 0.6278 - recall: 0.5290  9728/225000 [>.............................] - ETA: 2:25 - loss: 0.5292 - accuracy: 0.8226 - precision: 0.6344 - recall: 0.5264 10240/225000 [>.............................] - ETA: 2:24 - loss: 0.5211 - accuracy: 0.8254 - precision: 0.6405 - recall: 0.5235 10752/225000 [>.............................] - ETA: 2:23 - loss: 0.5124 - accuracy: 0.8280 - precision: 0.6464 - recall: 0.5207 11264/225000 [>.............................] - ETA: 2:23 - loss: 0.5051 - accuracy: 0.8307 - precision: 0.6520 - recall: 0.5184 11776/225000 [>.............................] - ETA: 2:23 - loss: 0.4977 - accuracy: 0.8325 - precision: 0.6575 - recall: 0.5163 12288/225000 [>.............................] - ETA: 2:22 - loss: 0.4895 - accuracy: 0.8358 - precision: 0.6628 - recall: 0.5143 12800/225000 [>.............................] - ETA: 2:22 - loss: 0.4824 - accuracy: 0.8373 - precision: 0.6677 - recall: 0.5120 13312/225000 [>.............................] - ETA: 2:22 - loss: 0.4754 - accuracy: 0.8391 - precision: 0.6725 - recall: 0.5097 13824/225000 [>.............................] - ETA: 2:21 - loss: 0.4689 - accuracy: 0.8401 - precision: 0.6771 - recall: 0.5077 14336/225000 [>.............................] - ETA: 2:20 - loss: 0.4615 - accuracy: 0.8413 - precision: 0.6816 - recall: 0.5059 14848/225000 [>.............................] - ETA: 2:20 - loss: 0.4549 - accuracy: 0.8430 - precision: 0.6859 - recall: 0.5041 15360/225000 [=>............................] - ETA: 2:19 - loss: 0.4476 - accuracy: 0.8457 - precision: 0.6900 - recall: 0.5022 15872/225000 [=>............................] - ETA: 2:19 - loss: 0.4410 - accuracy: 0.8478 - precision: 0.6939 - recall: 0.5001 16384/225000 [=>............................] - ETA: 2:18 - loss: 0.4368 - accuracy: 0.8488 - precision: 0.6976 - recall: 0.4979 16896/225000 [=>............................] - ETA: 2:18 - loss: 0.4312 - accuracy: 0.8498 - precision: 0.7011 - recall: 0.4957 17408/225000 [=>............................] - ETA: 2:17 - loss: 0.4260 - accuracy: 0.8516 - precision: 0.7045 - recall: 0.4937 17920/225000 [=>............................] - ETA: 2:17 - loss: 0.4213 - accuracy: 0.8527 - precision: 0.7078 - recall: 0.4917 18432/225000 [=>............................] - ETA: 2:19 - loss: 0.4162 - accuracy: 0.8547 - precision: 0.7110 - recall: 0.4898 18944/225000 [=>............................] - ETA: 2:19 - loss: 0.4114 - accuracy: 0.8558 - precision: 0.7141 - recall: 0.4877